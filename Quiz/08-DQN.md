# 08-DQN

1. 以下哪种方法是 DQN 的前身?

    - [x] Q-learning
    - [ ] SARSA
    - [ ] Monte Carlo 方法
    - [ ] Actor-Critic 方法

2. DQN 中使用哪种方法解决 Q 值估计的不稳定性问题?

    - [ ] 经验回放
    - [ ] 目标网络
    - [ ] A 和 B 都不是
    - [x] A 和 B 都是

3. 下列哪些算法是 DQN 基础上的改进算法? 【多选题】

    - [x] DDQN
    - [x] Dueling DQN
    - [ ] SAC
    - [ ] PPO

4. DQN 中的目标网络（Target Network）是用来解决什么问题的?

    - [ ] 网络的过拟合问题
    - [ ] 网络的梯度消失问题
    - [x] 网络的 Q 值估计不稳定问题
    - [ ] 网络的训练速度问题

5. DQN 中的经验回放是如何使用经验来解决 Q 值估计不稳定性问题的?

    - [ ] 在每次训练中只使用最新的经验
    - [x] 在训练时将之前的经验随机抽样并重复使用
    - [ ] 在训练时将之前的经验按时间顺序使用
    - [ ] 在训练时将之前的经验按优先级使用

6. DDQN 相对于 DQN 的改进就是首创了目标网络。

    - [ ] 对
    - [x] 错

7. Dueling DQN 的主要思想是什么?

    - [x] 对动作价值函数进行分解
    - [ ] 对状态价值函数进行分解
    - [ ] 对策略函数进行分解
    - [ ] 对优势函数进行分解

8. 优先经验回放相较于传统经验回放的优势主要在于提高学习效率。

    - [x] 对
    - [ ] 错

9. 在优先经验回放中，TD 误差越小的样本具有更高的优先级。

    - [ ] 对
    - [x] 错

10. 在 DQN 算法中，采用哪种方法来平衡探索和利用?

    - [x] Epsilon-greedy exploration
    - [ ] Softmax exploration
    - [ ] Upper confidence bound exploration
    - [ ] Thompson sampling

11. 下列关于 DQN 的说法中，正确的是? 【多选题】

    - [x] DQN 存在过高估计问题。
    - [ ] DQN 是一种同策略算法。
    - [x] DQN 中的经验回放用于打破样本之间的关联性。
    - [ ] DQN 可以用于处理连续动作空间问题。

12. DQN 算法使用贪心策略来选择动作，这个说法正确还是错误?
    - [ ] 对
    - [x] 错

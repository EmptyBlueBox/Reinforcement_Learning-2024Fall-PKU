# 10-策略梯度进阶

1. TRPO 算法属于什么类型的强化学习算法【多选题】

    - [x] policy-based 算法
    - [ ] value-based 算法
    - [x] on-policy 算法
    - [ ] off-policy 算法

2. 以下关于替代目标函数的说法中，错误的是

    - [ ] 替代目标函数刻画了更新后策略的累计期望收益
    - [ ] 替代目标函数可以看做是原目标函数的局部近似
    - [ ] 随着策略参数的更新，替代目标函数的形状会不断改变
    - [x] 基于替代目标函数进行优化，策略参数更新不会导致期望收益下降

3. 以下关于 TRPO 算法的说法中，正确的是

    - [ ] TRPO 属于一阶优化算法
    - [ ] TRPO 的 loss 函数中包含重要性采样比，因此 TRPO 是 off-policy 算法
    - [ ] TRPO 使用泰勒展开理论解，可以保证 KL 散度不超过指定阈值。
    - [x] TRPO 使用共轭梯度法，可以将空间复杂度降到 $O(n)$ 级别，其中 $n$ 为策略的参数量

4. PPO 算法属于什么类型的强化学习算法 【多选题】

    - [x] policy-based 算法
    - [ ] value-based 算法
    - [x] on-policy 算法
    - [ ] off-policy 算法

5. 以下关于 PPO 算法的说法中，错误的是

    - [ ] 属于一阶优化算法
    - [x] 能够实现策略收益的单调提升
    - [ ] PPO-penalty 版本会动态调整 KL 散度惩罚项的系数
    - [ ] PPO-clip 版本会对重要性采样比的范围进行约束

6. 以下关于 PPO 算法的实现中，正确的是

    - [ ] 必须使用 GAE 计算 advantage 函数
    - [x] GAE 的 lambda 参数取 0 时，advantage 函数与 TD 算法相同
    - [ ] 由于对重要性采样比进行了 clip，当学习率很大时，策略参数更新不会很大
    - [ ] entropy loss 的目的是促使模型激进地选择更好的动作

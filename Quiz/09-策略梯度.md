# 09-策略梯度

1. 基于策略的强化学习直接优化

    - [ ] 状态价值函数
    - [ ] 动作价值函数
    - [x] 策略函数
    - [ ] 优势价值函数

2. 下列关于 policy-based RL 的说法中，错误的有？

    - [ ] 可以学习随机策略
    - [ ] 可以用于连续动作空间
    - [ ] 本质上是一个优化问题
    - [x] 必须学习得到随机策略

3. 在 policy-based RL 中，主要的目标是什么？

    - [x] 最大化累计奖励
    - [ ] 最小化损失函数
    - [ ] 寻找最优价值函数
    - [ ] 无法确定

4. 在 policy-based RL 中，通常使用哪种算法来最大化策略的预期回报？

    - [x] 梯度上升法
    - [ ] 共轭梯度法
    - [ ] 拟牛顿法
    - [ ] 启发式优化方法

5. 减去一个常数或者只依赖于状态的基线会\_\_\_\_策略梯度的值。

    - [ ] 增大
    - [ ] 减小
    - [x] 不影响
    - [ ] 不确定

6. Actor-Critic 中 critic 的作用是什么？

    - [ ] 选择最优行动
    - [x] 估计当前状态价值
    - [ ] 估计转移概率
    - [ ] 输出策略梯度

7. 重参数化技巧相较于 REINFORCE 具有\_\_\_\_的方差。

    - [ ] 更大
    - [x] 更小
    - [ ] 同样
    - [ ] 不确定

8. Actor-Critic 算法中的 Actor 和 Critic 分别指代什么？

    - [x] Actor 指代策略，Critic 指代值函数
    - [ ] Actor 指代值函数，Critic 指代策略
    - [ ] Actor 和 Critic 都指代策略
    - [ ] Actor 和 Critic 都指代值函数

9. 以下哪个算法不是 Policy-based RL 算法？

    - [ ] REINFORCE
    - [x] Q-learning
    - [ ] Actor-Critic
    - [ ] Trust Region Policy Optimization (TRPO)

10. 在 policy-based RL 中，哪个技巧是直接用于防止策略梯度算法的方差过大的？

    - [x] 添加基线
    - [ ] 使用重要性采样
    - [ ] 使用 LSTM 网络
    - [ ] 增加训练数据

11. 下列关于重要性采样的说法中，哪些是正确的？【多选题】

    - [ ] 不改变数据的方差
    - [x] 可以用来估计不同分布下的期望值
    - [x] 采样分布必须保证在被估计的分布概率密度非 0 处非 0
    - [ ] 乘重要性权重之后的数据服从被估计的分布

12. REINFORCE 算法基于什么计算策略梯度？

    - [ ] TD 目标
    - [x] 累积回报
    - [ ] 动作价值函数
    - [ ] 状态价值函数

13. 有限差分方法需要目标函数可导。

    - [ ] 对
    - [x] 错

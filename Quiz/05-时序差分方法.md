# 05-时序差分方法

1. 下列关于时序差分方法的说法，哪个是正确的?

    - [ ] 在估计状态价值时，时序差分方法相较蒙特卡罗方法具有更高的方差
    - [ ] 时序差分方法是一种有模型强化学习方法
    - [x] 时序差分方法不需要知道完整轨迹信息

2. 同策略与异策略的区别在于目标策略与行为策略是否一致。

    - [x] 对
    - [ ] 错

3. 下列关于 Q-Learning 的说法中，哪一个是正确的?

    - [ ] Q-Learning 算法是一种基于模型的强化学习算法。
    - [ ] Q-Learning 算法可以用于解决连续状态和动作空间的问题。
    - [x] Q-Learning 算法使用 ε-贪心策略来平衡探索和利用。
    - [ ] Q-Learning 算法使用蒙特卡罗方法来更新 Q 值。

4. Q-Learning 中，TD 目标的后继状态的价值是如何计算的?

    - [ ] 后继状态的价值采用后继状态下所有动作的平均价值
    - [x] 后继状态的价值采用后继状态下最优动作的价值
    - [ ] 后继状态的价值采用后继状态下最差动作的价值
    - [ ] 后继状态的价值采用对后继状态进行完整采样得到的实际奖励

5. SARSA 算法中的“S”代表什么?

    - [x] State
    - [ ] Score
    - [ ] Strategy
    - [ ] Sequence

6. 下列哪个是蒙特卡洛控制算法?

    - [ ] SARSA
    - [ ] Q-Learning
    - [ ] 递归算法
    - [x] 蒙特卡洛策略迭代

7. MC 和 TD 有哪些共同点 【多选题】

    - [x] MC 方法和 TD 方法都可以用于求解价值函数
    - [x] MC 方法和 TD 方法都可以直接从经验中学习，不需要先对环境建立模型
    - [ ] MC 方法和 TD(0)都需要等到整个序列结束才能计算回报
    - [ ] MC 方法和 TD(0)都不需要等到整个序列结束才能计算回报

8. 下列哪些是无模型强化学习方法? 【多选题】
    - [x] Q-Learning
    - [x] SARSA
    - [ ] 策略迭代
    - [ ] 价值迭代

# 强化学习 2023 期末试题

1. 强化学习中的奖励（Reward）是指行为体做出行为后获得的

    - A. 正向反馈信号
    - B. 反馈信号
    - C. 对策略的评估
    - D. 对行为的评估

2. 强化学习的目标是让智能体在环境中

    - A. 累计奖励最大化
    - B. 即时奖励最大化
    - C. 能够完成环境定义的任务
    - D. 发现隐藏的结构并最大化利用

3. 基于策略的方法在训练过程中，网络输入状态后输出的是

    - A. 所有动作的价值函数
    - B. 状态的价值
    - C. 下一个动作的概率
    - D. 最优价值函数所对应的动作

4. Actor-Critic 架构中，Actor 负责，Critic 负责

    - A. 模拟环境，评估价值
    - B. 输出动作，评估价值
    - C. 输出动作，模拟环境
    - D. 评估价值，输出动作

5. 行为策略固定不变的学习方式属于

    - A. On-Policy 学习
    - B. Off-Policy 学习
    - C. TD 学习
    - D. MC 学习

6. 马尔可夫性质是给定当前状态后

    - A. 未来状态与当前状态是无关的
    - B. 未来状态与当前状态之后的状态是无关的
    - C. 未来状态与当前状态之前的状态是无关的
    - D. 当前状态与当前状态之后的状态是无关的

7. 在满足马尔可夫性质的环境中，S(t) 与 S(t+2)

    - A. 有关
    - B. 无关
    - C. 在给定 S(0) 的时候无关
    - D. 在给定 S(t+3) 的时候无关

8. 折扣因子等于零时

    - A. 回报等于即时奖励
    - B. 回报满足非负性
    - C. 回报无法被计算
    - D. 智能体会考虑到所有后续的奖励

9. 最优价值函数选择

    - A. 当前状态下即时奖励最高的动作
    - B. 当前状态下回报最高的动作
    - C. 当前状态下被探索次数最多的动作
    - D. 当前状态下探索次数最少的动作

10. 贝尔曼方程中的 $Q(s,a)$ 为

    - A. 状态价值函数
    - B. 动作价值函数
    - C. 状态-动作价值函数
    - D. 策略函数

11. Bootstrap 是指

    - A. 用累计回报作为状态估值
    - B. 用带折扣的累计回报作为状态估值
    - C. 以模型对后继状态价值的预测为依据，结合单步回报作为状态估值
    - D. 以模型对所有后续状态价值的预测为依据，结合单步回报作为状态估值

12. MC 算法与 TD 算法相比

    - A. MC 偏差优于 TD，MC 方差劣于 TD
    - B. MC 偏差劣于 TD，MC 方差劣于 TD
    - C. MC 偏差劣于 TD，MC 方差优于 TD
    - D. MC 偏差劣于 TD，MC 方差劣于 TD

13. 资格轨迹是指计算

    - A. 每个状态对 TD(λ) 的当前局面的影响的权重的方法
    - B. 每个状态在 MC 算法中对未来局面的影响的权重的方法
    - C. 每个状态在 n 步 TD 中对未来局面的影响的权重的方法
    - D. 每个状态在 n 步 TD 中对当前局面的影响的权重的方法

14. TD(λ) 是

    - A. 一种蒙特卡洛算法
    - B. 一种动态规划算法
    - C. Actor-Critic 方法
    - D. 一种时序差分算法

15. SARSA 算法中的 “S” 代表什么？

    - A. State（状态）
    - B. Score（分数）
    - C. Strategy（策略）
    - D. Sequence（序列）

16. TD(λ) 的后向视角

    - A. 是一步计算某一个状态所有加权收益的方法
    - B. 是相对前向视角效率更高的计算方式
    - C. 每个状态的收益经计算后都不用再更新的方法
    - D. 计算后总会导致状态出现误差，需要资格轨迹的帮助以修正

17. 在使用每次蒙特卡洛方法更新值函数时，如果一个状态在回合中被访问 2 次，那么它的值函数会被更新几次？

    - A. 0
    - B. 1
    - C. 2
    - D. 3

18. TD(λ) 算法中的 λ 参数的取值范围是：

    - A. $[0,1]$
    - B. $(0,1)$
    - C. $[0,1)$
    - D. $(0,1]$

19. 下列关于 Q-Learning 的说法中，哪一个是正确的？

    - A. Q-Learning 算法是一种基于模型的强化学习算法。
    - B. Q-Learning 算法可以用于解决连续状态和动作空间的问题。
    - C. Q-Learning 算法使用 ε-贪心策略来平衡探索和利用。
    - D. Q-Learning 算法使用蒙特卡罗方法来更新 Q 值。

20. MC 方法和 TD 方法之间有哪些共同点？

    - A. MC 方法和 TD(0) 都需要等到整个序列结束才能计算回报
    - B. MC 方法和 TD(0) 都不需要等到整个序列结束才能计算回报
    - C. MC 方法和 TD 方法都可以直接从经验中学习，不需要先对环境建立模型
    - D. MC 方法和 TD 方法都是一种策略方法

21. 以下关于 TD 和 MC 估值算法的说法中，正确的是？

    - A. TD 和 MC 都是无偏估计
    - B. TD 和 MC 都是有偏估计
    - C. TD 是有偏估计，MC 是无偏估计
    - D. TD 是无偏估计，MC 是有偏估计

22. 以下关于 TD 和 MC 估值算法的说法中，错误的是？

    - A. 非马尔科夫的环境更适合使用 TD 而非 MC 估值
    - B. MC 估值算法需要完整的状态动作序列来学习值函数
    - C. 使用线性函数时，MC 估值算法保证值函数收敛到全局最优
    - D. 使用线性函数时，TD 估值算法不保证收敛到全局最优

23. 以下关于 DQN 的说法中，错误的是？

    - A. DQN 是一种非梯度算法
    - B. DQN 是一种 TD 算法
    - C. DQN 估计的是状态-动作值函数
    - D. DQN 会产生高估现象

24. 以下关于 DQN 的说法中，正确的是？

    - A. DQN 是 On-policy 的算法
    - B. DQN 算法的网络输入为状态和离散的动作，输出为该状态动作对的估值
    - C. DQN 中采用经验回放池可以打破数据之间的关联，使训练更稳定
    - D. Double DQN 中提出了优势函数的概念来区分辨当前价值是状态还是动作带来的

25. 以下关于 DQN 的说法中，正确的是？

    - A. DQN 通过向损失函数添加熵正则项来鼓励探索
    - B. Dueling DQN 的 Q 值表示为两项之和：状态价值函数与状态动作优势函数
    - C. 优先经验回放主要解决 DQN 高估的问题
    - D. 优先经验回放通过调整学习率来修正优先级带来的关联性

26. 以下哪个 DQN 变种不包含于 RAINBOW 中？

    - A. Nash DQN
    - B. Prioritized replay DQN
    - C. Distributional DQN
    - D. Multi-step DQN

27. 以下关于多臂老虎机的说法中，正确的是？

    - A. 如果更新步长取常数，那么计算出来的动作价值估计为历史动作收益的算术平均值
    - B. 如果使用 ε-贪心探索，当 ε 越小时收益越慢，且收益后的值越小
    - C. 乐观初值贪心可以有效处理环绕非固定的问题
    - D. UCB 算法通过在动作收益中添加探索程度项来鼓励探索

28. 以下对无梯度的启发式优化方法描述错误的是？

    - A. 有限差分的基本思想是在参数上添加微扰，从而估计目标函数的梯度
    - B. 有限差分方法要求策略可微
    - C. 交叉熵方法不断向策略生成的最优秀的数据学习，最终收敛到最优策略
    - D. 交叉熵方法适用于回合较短、奖励稀疏的简单环境

29. 以下哪种算法不是基于策略的算法？

    - A. SARSA
    - B. REINFORCE
    - C. DDPG
    - D. PPO

30. 关于策略梯度算法的描述中正确的是？

    - A. REINFORCE 算法中无需应用策略梯度定理
    - B. 策略梯度定理将策略的梯度表示为基于采样的近似公式
    - C. REINFORCE 算法没有收敛性保证
    - D. 引入状态基线值会改变策略梯度估计值

31. 关于 Actor-Critic 算法的描述中错误的是？

    - A. 引入状态基线值可以降低策略梯度估计值的方差
    - B. 将含状态基线值的 REINFORCE 算法中的蒙特卡洛估值换为 TD(0) 可以得到单步
      Actor-Critic
    - C. Actor-Critic 算法中 Critic 使用策略梯度做提升
    - D. Actor-Critic 算法中 Actor 用于训练策略，找出最优动作

32. 下列关于重要性采样的说法中，错误的是？

    - A. 会改变数据的方差
    - B. 可以用于估计不同分布下的期望值
    - C. 乘以重要性权重后，数据服从被估计的分布
    - D. 可以用于估计不同分布下数据的平方的期望值

33. 以下关于 TRPO 算法的说法中，正确的是？

    - A. TRPO 属于一阶优化算法
    - B. TRPO 的 loss 函数中包含重要性采样比，因此 TRPO 是 off-policy 算法
    - C. TRPO 使用线搜索来为了计算 Hessian 矩阵
    - D. TRPO 使用共轭梯度法，能将空间复杂度降到 $O(n^2)$ 级别，其中 $n$ 为策略的参数量

34. 以下关于替代目标函数的说法中，正确的是？

    - A. 随着策略参数的更新，替代目标函数的形状不会改变
    - B. 基于替代目标函数进行优化，策略参数更新可能会导致期望收益下降
    - C. 替代目标函数刻画了更新后策略的单步期望收益
    - D. 替代目标函数不能看做是原目标函数的局部近似

35. 以下关于 PPO 算法的说法中，错误的是？

    - A. PPO-penalty 版本会动态调整 KL 散度惩罚项的系数
    - B. PPO-penalty 版本主要思想是将非负约束视为一种奖励机制
    - C. PPO-clip 版本动作概率分布的变化不能直接反映策略的变化
    - D. PPO-clip 版本会对重要性采样比的范围进行约束

36. 下列关于 PPO 算法的说法中，错误的是？

    - A. PPO 是一种一阶优化算法
    - B. PPO-penalty 通过在 loss 上添加惩罚项将 TRPO 的硬性约束改为软约束
    - C. PPO 允许在 KL 散度上添加 clip 来限制策略更新幅度
    - D. PPO 不一定单调提升

37. 关于 PPO 算法的说法中，错误的是？

    - A. 使用共享主干上的值网络与策略网络可以降低学习难度
    - B. PPO 是一种 Actor-Critic 算法
    - C. loss 函数中的熵正则项用于鼓励探索
    - D. 使用较大的学习率，一个 batch 仅做一次更新可以有效避免陷入平台

38. 重参数化技巧解决什么问题？

    - A. 计算图中包含随机变量导致梯度无法反向传播
    - B. 计算图中包含方差过大的随机变量导致梯度爆炸
    - C. 计算图中包含均值为 0 的随机变量导致梯度消失
    - D. 通过采样估计策略梯度

39. 在 policy-based RL 中，通常使用什么算法来最大化策略的预期回报？

    - A. 共享梯度法
    - B. 拟牛顿法
    - C. 启发式优化算法
    - D. 梯度上升法

40. 在函数拟合中，过拟合指的是什么？

    - A. 模型损失函数达到局部最优解
    - B. 模型参数数量多，导致训练困难
    - C. 模型过于复杂，导致在训练集上表现很好，测试集上表现很差
    - D. 模型过于简单，无法捕捉到数据中的复杂关系

41. 下列关于随机梯度下降算法的说法中正确的是？

    - A. 是一种使用了动量方法优化的梯度下降算法
    - B. 学习率过大的时候难以收敛，会出现震荡现象
    - C. 应当设置较大的初始学习率值，以加快初始学习速度
    - D. 应当设置较小的初始学习率值，以防止过拟合

42. DQN 中的经验回放是什么？

    - A. 按照优先级依次使用数据训练
    - B. 在每次训练时只使用最新的数据
    - C. 在训练时将数据随机抽样并重复使用
    - D. 在训练时按照时间顺序依次使用数据

43. 下列关于 GAE 算法的描述中，错误的是？

    - A. 是一种计算优势函数数值估值的方法
    - B. λ 越大，估计的偏差越大
    - C. λ 越小，估计的方差越小
    - D. λ=0 时，GAE 的形式就是 TD 误差

44. 下列关于 DDPG 算法的叙述中，错误的是？
    - A. 适用于连续动作问题
    - B. 使用了 Actor-Critic 架构，并且使用目标网络稳定学习过程
    - C. 仅用于经验回放技术与软更新方法
    - D. 是一种 On-Policy 算法

45. 下列关于 DDPG 算法的叙述中，正确的是？

    - A. 使用确定性的方式实现探索
    - B. 策略网络输出的动作分布的均值和方差
    - C. 使用了带噪声的探索策略
    - D. 软更新仅针对策略网络，不针对策略网络

46. 以下关于分布式机器学习数据并行的说法中，错误的是

    - A. 每个节点只存储一部分模型参数
    - B. 各节点处理的输入数据不同
    - C. 各节点分列计算模型参数的梯度
    - D. 参数更新按下各节点的平均梯度

47. 以下关于分布式机器学习中流水并行的说法，正确的是

    - A. 每个节点需要存储完整的模型参数
    - B. 一组数据需要依次被多个节点处理
    - C. 在 PipeDream 流水线方案中，不同节点不可能同时进行前向传播和反向传播
    - D. Gpipe 方案比 PipeDream 方案更高效

48. 以下关于分布式强化学习的说法中，错误的是

    - A. A3C 算法中，Actor 通过反向传播计算出梯度，发送给 Learner 进行梯度更新
    - B. IMPALA 算法中，Actor 负责采集整局的样本数据发送给 Learner
    - C. SEED RL 算法中，Learner 需要将更新后的模型参数传递给各 Actor
    - D. DD-PPO 算法中，不同 Worker 之间需要对梯度计算全局的平均

49. 以下分布式强化学习算法中，由 Learner 进行模型同步传播的是

    - A. A3C
    - B. IMPALA
    - C. SEED RL
    - D. DD-PPO

50. 当模型参数量远大于样本量时，以下算法中各节点间通信代价最低的是

    - A. A3C
    - B. IMPALA
    - C. SEED RL
    - D. DD-PPO

51. 以下关于博弈论 self-play 相关算法的描述中，错误的是

    - A. Fictitious Self-Play 算法将 Fictitious Play 算法扩展到了多回合游戏上
    - B. Neural Fictitious Self-play 算法中，可以训练值网络来拟合历史平均策略
    - C. Double Oracle 算法不断计算出当前策略集合的 Nash 均衡，并将其最优应对添加到该集合中
    - D. PSRO 算法对每个玩家维护策略集合同，训练出特定元策略的最优应对添加到集合中

52. 以下强化学习对抗博弈算法的描述中，错误的是

    - A. naive self-play 算法直接用最新模型自对战产生数据
    - B. delta-uniform self-play 算法随机选择最新的若干模型作为对手产生对战数据
    - C. prioritized self-play 算法使用历史模型中最优的一个自对战产生数据
    - D. population-based self-play 算法维护多个种群的对手，选择策略不同的对手

53. 以下多智能体强化学习中，哪个算法可以解决智能体之间存在竞争的问题

    - A. VDN
    - B. QMIX
    - C. COMA
    - D. MADDPG

54. 以下关于多智能体强化学习算法的说法中，正确的是

    - A. VDN 算法中每个 agent 只使用自身的 Q 网络进行实际决策
    - B. QMIX 算法认为团队收益可以简单拆解成各 agent 个人收益的和
    - C. COMA 算法将 agent 实际动作替换成最差动作，用值函数的减少量作为该 agent 的团队贡献值
    - D. MADDPG 算法中每个 agent 的 critic 网络只输入自己的观测数据；并用自己的 reward 训练

55. 以下关于围棋 AI AlphaGo 的说法中，错误的是

    - A. 两个策略网络的训练都使用了监督学习
    - B. 其复杂的策略网络的强化学习训练提升了它的水平
    - C. 简单的策略网络被用于 MCTS 算法中搜索树上子节点的选择
    - D. 估值网络的训练使用了复杂策略网络生成的对局数据集

56. 以下关于德扑 AI AlphaHoldem 的说法中，错误的是

    - A. 对游戏状态的特征编码采用图像形式，使用卷积神经网络提取特征
    - B. 将策略网络和值网络合并为一个网络同时训练
    - C. 训练时采用 naive self-play 的自对战策略
    - D. 使用 Trinal-Clip PPO 算法，在值网络的 loss 中对 reward 进行 clip

57. 以下关于日麻 AI Suphx 的说法中，错误的是

    - A. 基于人工构建的决策流程训练了多个策略网络
    - B. 强化学习训练时采用 naive self-play 的自对战策略
    - C. 使用 PID 算法处理非完美信息，critic 接受全局信息，actor 只接受可见信息
    - D. 额外训练 global reward predictor 用于生成每一小局的 reward

58. 以下关于斗地主 AI DouZero 和 PerfectDou 的说法中，错误的是

    - A. DouZero 基于 DQN 算法，训练 Q 网络预测状态动作价值
    - B. DouZero 对两个农民和一个地主训练了三个价值网络
    - C. PerfectDou 中，策略网络和价值网络的输入数据相同
    - D. PerfectDou 使用 reward shaping，通过引入打完手牌的最少次数来判断局面好坏

59. 以下关于星际争霸 AI AlphaStar 的说法中，错误的是

    - A. 先监督学习训练初始模型，再使用强化学习接续训练
    - B. 对游戏中的每个种族训练一套不同的模型参数
    - C. 在自对战训练中，main agent 的对手从所有种群中进行采样
    - D. 在自对战训练中，league exploiter 只以 main agent 为对手进行训练

60. 以下关于王者荣耀 AI 绝悟的说法中，错误的是：

    - A. 基于 Dual-Clip PPO 算法，对重要性样本比增加额外的 clip
    - B. 自对战训练时，80% 从历史模型中随机采样，20% 使用最新模型作为对手
    - C. 先基于固定的英雄组合训练多个模型，再通过知识蒸馏学习不同组合下的通用策略
    - D. 在选英雄阶段使用 MCTS 算法搜索最优动作

# 王者荣耀大作业报告

## 特征处理 & 训练流程

主要就是 `train_workflow.py` 文件：

1. 在 `workflow` 函数中，通过 `run_episodes(envs, agents, logger, monitor)` 不断启动新的对局。
2. `run_episodes` 函数中，通过一个外层的 `while True` 来不断进行新的对局，生成每一局的数据 `g_data`，然后再在内层的 `while True` 中不断推进对局，直到对局结束。
3. 在这个 `while` 循环内，调用 `env.step` 来推进到下一帧，若当前非评估局，则会利用 `env` 给出的 `state_dicts` 和 agent 自身所维护的 `obs_data`、`act_data` ，调用 `build_frame` 创建一个帧（即 `Frame` 类），这个类中包括了学习所需要的全部信息。
4. 但这个样本不会被立刻送给 agent 学习，而是暂存于 `frame_collector`，且依照帧号顺序保存。等到当前对局正常结束（非 `truncated`）后才会调用 `sample_process` 函数对值进行处理。
5. 在 `sample_process` 中，每处理完 `self._LSTM_FRAME` 个样本帧数据时，会将之与 LSTM 状态等数据整合为一个完整的训练样本，转换得到样本集合 `list_agents_samples`，然后通过 `yield` 送入经验回放样本池，供后续 `agent.learn` 训练使用。

## 奖励函数

在训练流程中，奖励函数的计算通过

```python
reward = agents[i].reward_manager.result(state_dicts[i]["frame_state"])
```

执行，而奖励的管理依赖于 `reward_manager.py`，其中定义了 `GameRewardManager` 类与 `set_cur_calc_frame_vec` 、`get_reward` 函数。

`set_cur_calc_frame_vec` 函数主要用于计算并更新指定奖励字典 `cul_calc_frame_map` 中各个奖励子项。它为我们后续计算奖励提供了数值基础，这里我们参照开源代码，在基础代码的实现上增加了多个奖励项，以充分利用环境与观测信息，这些新增的奖励项包括：

- `close_to_cake`：靠近血包。我们认为，靠近血包对于维持血量是有益的。
- `extra_mov_spd`：移速。主要是为了激励 Agent 通过装备等手段提升移速，便于攻击。
- `kill_monster`：击杀野怪。主要是希望能通过击杀野怪增加经验、金钱。
- `hit_target`：命中目标。综合多个属性判断英雄技能是否成功命中敌方英雄，并减去自身收到的敌方英雄伤害来鼓励准确命中且尽量减少受伤。
- `kiting`：拉扯。鼓励保持合理的英雄距离，从而降低受敌方英雄攻击的风险。这一项还会考虑英雄的攻击范围。
- `hit_by_organ`：被防御塔攻击。引导 Agent 远离敌方防御塔，通过判断防御塔所属阵营与是否以自身为攻击目标实现。

对主英雄和敌对英雄都完成 `set_cur_calc_frame_vec` 计算后，就会调用 `get_reward` 函数来对各个奖励项进行加权求和，给出总的奖励。

这里，与初始代码包的全局奖励权重不同，我们会先获取当前帧数据中的帧号 `frame_no`，然后根据其判断游戏所处的阶段：

- $\text{frame\_no} < 3600$：阶段 1
- $3600 \leq \text{frame\_no} < 12000$：阶段 2
- $12000 \leq \text{frame\_no}$：阶段 3

在一局游戏不同阶段，我们会使用不同的 `REWARD_WEIGHT_DICT`，在其间进行动态调整，使得奖励权重能根据游戏阶段和时间变化进行适应性改变，逐渐降低所有的非必要性奖励，以引导 Agent 在不同阶段的注意力，如：

- 在最早期，`hp_rate` 和 `tower_hp_point` 之间差距不算太大，但当游戏进入到中期，则会使得后者高过前者一个数量级，同时降低 `hit_by_organ`，容忍 Agent 为了推塔付出一定的代价。
- 在整个过程，逐渐降低如 `money` `exp` `close_to_cake` `last_hit` 等权重，因为随着游戏的进行，这些奖励对于推塔的重要性逐渐减弱，并且血量充足且等级达到要求时，`forward` 奖励会显著增加，但是血量不足时，`forward` 奖励会显著降低。

## 模型结构

Agent 的模型在 `model.py` 中的 `Model` 类实现。这部分相较于原始代码包的改动主要是为了支持奖励函数的修改，额外添加了对应的特征维度与处理。

模型的输入在 `agent.py` 的 `_model_inference` 中实现，首先从 `list_obs_data` 解包出各个 `obs_data` 的信息，包括 `feature_vec` `lstm_hidden_state` `lstm_cell_state`，然后再把各个 `obs_data` 的信息拼在一起作为 `format_inputs`，送入模型进行前向推理。

在 `forward` 函数里，首先会将输入的 `feature_vec` 按照不同的特征维度进行切分，得到英雄、士兵、建筑、全局信息等多个部分。对于每个部分，我们都设计了专门的处理模块：

- 英雄模块：分为主英雄(main_hero)和敌方英雄(enemy_hero)三部分。每部分都会经过一个MLP进行特征提取，其中友方和敌方英雄还会额外经过一个FC层，并且会将部分输出作为目标嵌入(target embedding)保存。

- 士兵模块：分为友方士兵和敌方士兵。每个士兵都会经过相同的MLP和不同的FC层，然后将同阵营的士兵特征拼接并reshape成固定维度。

- 建筑模块：类似士兵的处理方式，分为友方建筑和敌方建筑，使用共享的MLP和独立的FC层，最后进行拼接和维度变换。

- 其他模块：包括野怪(monster)、血包(cake)、子弹(bullet)、buff、技能(command)和目标信息(target)等，每个模块都有独立的MLP进行特征提取。

所有模块的输出会被拼接在一起形成 `concat_result`，送入公共连接层 `concat_mlp` 进行特征融合。同时，模型使用LSTM来处理时序信息，将融合后的特征和上一时刻的隐藏状态(`lstm_hidden_init`)、细胞状态(`lstm_cell_init`)一起输入LSTM，得到当前时刻的输出和更新后的状态。

模型还包含一个并行的 `parallel_mlp` 分支，处理相同的融合特征。两个分支的输出会被拼接在一起，经过 `label_mlp` 计算动作概率分布。此外，模型还实现了注意力机制，通过 `lstm_tar_embed_mlp` 和矩阵乘法操作，让模型能够关注到重要的目标。最后通过 `value_mlp` 计算状态价值估计。

所有这些输出会被整合成一个列表 `rst_list` 作为模型的最终输出。在训练时，模型使用PPO算法的目标函数计算损失。具体来说，损失函数包含三个部分：

1. Value Loss (价值损失)：用于评估模型对状态价值估计的准确性，通过比较预测的价值和实际的回报计算MSE损失。

2. Policy Loss (策略损失)：基于PPO的裁剪目标函数，通过比较新旧策略的比率来限制策略更新的幅度，防止过大的策略变化。

3. Entropy Loss (熵损失)：添加策略熵作为正则项，乘以系数 `var_beta` 来控制探索程度。较大的熵意味着动作选择更加随机，有助于在训练早期保持充分的探索。

这三个损失按照一定权重组合：

```python
self.loss = self.value_cost + self.policy_cost + self.var_beta * self.entropy_cost
```

## Agent 实现

Agent 实现在 `agent.py` 中，主要修改了 `observation_process` 函数来实现前文所述奖励项的观测计算。具体来说:

1. 状态信息提取
- 从 `state_dict` 中获取当前帧的状态信息，包括英雄列表、帧号、NPC列表等
- 通过 `player_id` 找到主英雄(狄仁杰)和敌方英雄
- 获取英雄的阵营信息，用于后续的坐标镜像处理

2. 英雄特征向量(6维)
- 使用 CONFIG_ID_MAP 将狄仁杰(config_id=133)映射到特征向量的第一个位置
- 在主英雄位置设为1，其他位置为0
- 敌方英雄同理，但偏移3个位置
- 这种one-hot编码可以让模型清晰识别英雄类型

3. 野怪信息特征(14维)
- 获取中立野怪的详细属性:
  - 位置坐标(x,z)
  - 朝向向量(x,z)
  - 血量百分比：当前血量/最大血量
  - 物理攻击力和防御力
  - 魔法攻击力和防御力
  - 移动速度
  - 攻击范围和目标
  - 击杀收益
  - 视野范围
- 如果主英雄是2号阵营，所有坐标需要乘以-1进行镜像

4. 血包信息特征(2维)
- 遍历场上所有血包
- 根据血包x坐标和英雄阵营判断归属:
  - x<0时，PLAYERCAMP_1阵营为己方血包
  - x>0时，PLAYERCAMP_2阵营为己方血包
- 分别记录己方和敌方血包的存在情况

5. 子弹信息特征(30维)
- 专门记录狄仁杰的三个技能:
  - 一技能(弹道)：记录子弹位置和数量
  - 二技能(闪避)：记录位移轨迹
  - 三技能(狙击)：记录狙击方向和范围
- 记录防御塔的攻击:
  - 分别记录一塔和二塔的子弹位置
  - 记录子弹数量用于判断攻击强度
- 所有坐标都需要根据阵营进行镜像处理

6. BUFF信息特征
- 记录狄仁杰特有的BUFF:
  - 被动标记：增加暴击率
  - 二技能加速效果
  - 装备提供的各类BUFF
- 同时记录敌方英雄的关键BUFF
- 使用buff_id_map进行映射，未知BUFF会被记录到extra_set

7. 英雄状态特征
- 记录狄仁杰的攻击目标，分为8类:
  - 近战兵、远程兵、炮车
  - 外塔、内塔
  - 野怪、英雄
  - 阵亡单位
- 记录英雄朝向角度：需要根据阵营进行镜像
- 记录当前行为模式：移动、停止、普攻、技能等

所有这些特征都经过精心设计和归一化处理，以便神经网络更好地学习。特别针对狄仁杰这个英雄，我们重点关注了:
1. 技能机制：通过详细的子弹信息让模型学会技能释放时机
2. 位置控制：通过坐标系统让模型学会保持安全距离
3. 资源获取：通过野怪和血包信息让模型学会资源控制
4. 目标选择：通过攻击目标系统让模型学会优先级判断

这些特征的设计充分考虑了狄仁杰作为一个远程射手的特点，帮助模型更好地发挥他的优势。

## 对我们所做的尝试进行总结

1. 调参数，尤其是奖励函数的权重
2. 对 codebase 的修改
3. 堆算力, 不停续训模型
4. 模型按照强弱排序并非偏序
   1. 这是因为每个模型都有自己的优势和劣势，如果某个模型恰好在某个进攻策略上没有防备，那么它相比具备这个策略的模型就会显得很弱。

## 任务分工

卓致用：
1. 奖励函数调整
2. 课堂汇报 Slide 制作
3. 阅读代码报告撰写

梁昱桐：
1. 模型训练比较
2. 奖励函数调整
3. 报告撰写
